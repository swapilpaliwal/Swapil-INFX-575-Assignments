{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Code For LDA on 10 patent documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top ten words and their frequencies\n",
      "Topic 1 :\n",
      "0.063*protector + 0.050*strap + 0.036*section + 0.035*first + 0.032*nozzl + 0.032*rotari + 0.028*second + 0.027*jet + 0.026*assembl + 0.020*whirlpool\n",
      "\n",
      "Topic 2 :\n",
      "0.078*cm/ + 0.068*accord + 0.068*measur + 0.057*rate + 0.052*method + 0.046*flow + 0.042*sleep + 0.042*airflow + 0.039*air + 0.037*area\n",
      "\n",
      "Topic 3 :\n",
      "0.109*bag + 0.085*sleep + 0.050*part + 0.050*bodi + 0.039*leg + 0.035*main + 0.035*foot + 0.032*substanti + 0.024*user + 0.022*drawstr\n",
      "\n",
      "Topic 4 :\n",
      "0.100*edg + 0.058*perineum + 0.050*protect + 0.050*devic + 0.049*gener + 0.045*panel + 0.042*inch + 0.039*first + 0.035*second + 0.034*loop\n",
      "\n",
      "Topic 5 :\n",
      "0.056*member + 0.044*faucet + 0.036*tighten + 0.036*support + 0.031*bathtub + 0.026*seat + 0.025*plural + 0.024*defin + 0.023*base + 0.021*connector\n"
     ]
    }
   ],
   "source": [
    "# nltk package is used for basic natural language processing operations such as removing stopwords, tokenizing words, stemming and\n",
    "# lemmatization.\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# gensim package is applied for LDA modelling and creation of term frequency - inverse document matrices\n",
    "from gensim import corpora \n",
    "from gensim.models import ldamodel\n",
    "from gensim.models import tfidfmodel \n",
    "\n",
    "word_list = []\n",
    "\n",
    "# selecting the 10 patent documents \n",
    "for i in range(6334220,6334229):\n",
    "    string = str(i)\n",
    "    paragraph = open(string +'.txt','r')\n",
    "    \n",
    "    for text in paragraph:\n",
    "        structure = text\n",
    "        \n",
    "    # Creating Tokens using word_tokenize function in nltk library\n",
    "    mysentencetokens_sw= nltk.word_tokenize(str(structure))\n",
    "\n",
    "    # normazlizing everything to lower case\n",
    "    looper = 0\n",
    "    for token in mysentencetokens_sw:\n",
    "        mysentencetokens_sw[looper] = token.lower()\n",
    "        looper += 1\n",
    "\n",
    "    # removing stopwords using nltk library with anything less than 2 letters\n",
    "    minlength = 2\n",
    "    mysentencetokens = [token for token in mysentencetokens_sw if \n",
    "                    (not token in stopwords.words('english')) \n",
    "                    and len(token) > minlength]\n",
    "    \n",
    "    #Transforming the words into the stem forms using PorterStemmer function from nltk library\n",
    "    porter = nltk.PorterStemmer()\n",
    "    looper = 0\n",
    "    for token in mysentencetokens:\n",
    "        mysentencetokens[looper] = porter.stem(token)\n",
    "        looper += 1\n",
    "    \n",
    "    #lemmatizing the words so that they are known words from the dictionary using the WordNetLemmatizer function\n",
    "    lmtzr = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "    looper = 0\n",
    "    for token in mysentencetokens:\n",
    "        mysentencetokens[looper] = lmtzr.lemmatize(token)\n",
    "        looper += 1\n",
    "        word_list.append(mysentencetokens)\n",
    "\n",
    "#creating a Dictionary and Corpus from the 10 patent documents\n",
    "dictionary = corpora.Dictionary(word_list) \n",
    "corpus = [dictionary.doc2bow(words) for words in word_list] \n",
    "\n",
    "# creating the term document frequency matrix to obtain distribution of words in a topic across the documents in the corpus\n",
    "term_document_frequency_matrix = tfidfmodel.TfidfModel(corpus) \n",
    "new_corpus = term_document_frequency_matrix[corpus] \n",
    "\n",
    "# applying the LDA model, setting the number of topics to 5 and printing top 10 words on each topic \n",
    "# (Note : default value is top ten words for show_topics command)\n",
    "find_topics = ldamodel.LdaModel(new_corpus, id2word=dictionary, num_topics=5, passes = 20) \n",
    "topics = find_topics.show_topics(formatted = True) \n",
    "\n",
    "print \"Top ten words and their frequencies\" \n",
    "print \"Topic 1 :\" \n",
    "print topics[0]\n",
    "print\n",
    "\n",
    "print \"Topic 2 :\" \n",
    "print topics[1]\n",
    "print\n",
    "\n",
    "print \"Topic 3 :\" \n",
    "print topics[2]\n",
    "print\n",
    "\n",
    "print \"Topic 4 :\" \n",
    "print topics[3]\n",
    "print\n",
    "\n",
    "print \"Topic 5 :\"\n",
    "print topics[4]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Output of above code (Part 1)\n",
    "\n",
    "Top ten words and their frequencies\n",
    "Topic 1 :\n",
    "0.063*protector + 0.050*strap + 0.036*section + 0.035*first + 0.032*nozzl + 0.032*rotari + 0.028*second + 0.027*jet + 0.026*assembl + 0.020*whirlpool\n",
    "\n",
    "Topic 2 :\n",
    "0.078*cm/ + 0.068*accord + 0.068*measur + 0.057*rate + 0.052*method + 0.046*flow + 0.042*sleep + 0.042*airflow + 0.039*air + 0.037*area\n",
    "\n",
    "Topic 3 :\n",
    "0.109*bag + 0.085*sleep + 0.050*part + 0.050*bodi + 0.039*leg + 0.035*main + 0.035*foot + 0.032*substanti + 0.024*user + 0.022*drawstr\n",
    "\n",
    "Topic 4 :\n",
    "0.100*edg + 0.058*perineum + 0.050*protect + 0.050*devic + 0.049*gener + 0.045*panel + 0.042*inch + 0.039*first + 0.035*second + 0.034*loop\n",
    "\n",
    "Topic 5 :\n",
    "0.056*member + 0.044*faucet + 0.036*tighten + 0.036*support + 0.031*bathtub + 0.026*seat + 0.025*plural + 0.024*defin + 0.023*base + 0.021*connector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Creation of toy data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documents labelled \"doc1.txt\", \"doc2.txt\" and \"doc3.txt\" are based on nanotechnology and related subjects.                     \n",
    "Documents labelled \"doc4.txt\" and \"doc5.txt\" are based on datascience and databases. \n",
    "\n",
    "Doc1\n",
    "\n",
    "Materials science, also commonly known as materials science and engineering, is an interdisciplinary field which deals with the discovery and design of new materials. Though it is a relatively new scientific field that involves studying materials through the materials paradigm (synthesis, structure, properties and performance), its intellectual origins reach back to the emerging fields of chemistry, mineralogy and engineering during the Enlightenment. It incorporates elements of physics and chemistry, and is at the forefront of nanoscience and nanotechnology research. In recent years, materials science has become more widely known as a specific field of science and engineering.\n",
    "\n",
    "Doc2\n",
    "\n",
    "Nanoparticles are particles between 1 and 100 nanometers in size. In nanotechnology, a particle is defined as a small object that behaves as a whole unit with respect to its transport and properties.  Particles are further classified according to diameter. Nanoparticles are of great scientific interest as they are, in effect, a bridge between bulk materials and atomic or molecular structures. A bulk material should have constant physical properties regardless of its size, but at the nano-scale size-dependent properties are often observed. \n",
    "\n",
    "Doc3\n",
    "\n",
    "Nanotechnology as defined by size is naturally very broad, including fields of science as diverse as surface science, organic chemistry, molecular biology, semiconductor physics, microfabrication, etc. The associated research and applications are equally diverse, ranging from extensions of conventional device physics to completely new approaches based upon molecular self-assembly, from developing new materials with dimensions on the nanoscale to direct control of matter on the atomic scale.\n",
    "\n",
    "Doc4\n",
    "\n",
    "In general terms, Data Science is the extraction of knowledge from data. It employs techniques and theories drawn from many fields within the broad areas of mathematics, statistics, information theory and information technology, including signal processing, probability models, machine learning, statistical learning, computer programming, data engineering, pattern recognition and learning, visualization, predictive analytics, uncertainty modeling, data warehousing, data compression and high performance computing. Methods that scale to Big Data are of particular interest in data science, although the discipline is not generally considered to be restricted to such data.\n",
    "\n",
    "Doc5\n",
    "\n",
    "Formally, a \"database\" refers to a set of related data and the way it is organized. Access to this data is usually provided by a \"database management system\" (DBMS) consisting of an integrated set of computer software that allows users to interact with one or more databases and provides access to all of the data contained in the database (although restrictions may exist that limit access to particular data). The DBMS provides various functions that allow entry, storage and retrieval of large quantities of information as well as provides ways to manage how that information is organized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 (A): LDA on toy Data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(140 unique tokens: [u'particular', u'size-depend', u'comput', u'discoveri', u'enlighten']...)\n",
      "Top ten words and their frequencies\n",
      "Topic 1 :\n",
      "0.126*data + 0.048*learn + 0.032*inform + 0.032*theori + 0.032*model + 0.032*gener + 0.032*statist + 0.032*comput + 0.016*within + 0.016*drawn\n",
      "\n",
      "Topic 2 :\n",
      "0.007*field + 0.007*scienc + 0.007*nanotechnolog + 0.007*physic + 0.007*materi + 0.007*perform + 0.007*engin + 0.007*broad + 0.007*includ + 0.007*scale\n",
      "\n",
      "Topic 3 :\n",
      "0.078*particl + 0.053*bulk + 0.053*nanoparticl + 0.037*properti + 0.029*size + 0.027*unit + 0.027*whole + 0.027*transport + 0.027*accord + 0.027*diamet\n",
      "\n",
      "Topic 4 :\n",
      "0.047*known + 0.040*materi + 0.030*engin + 0.026*chemistri + 0.026*new + 0.024*year + 0.024*wide + 0.024*mineralog + 0.024*nanosci + 0.024*paradigm\n",
      "\n",
      "Topic 5 :\n",
      "0.053*diver + 0.031*molecular + 0.027*equal + 0.027*devic + 0.027*semiconductor + 0.027*self-assembl + 0.027*rang + 0.027*organ + 0.027*natur + 0.027*nanoscal\n"
     ]
    }
   ],
   "source": [
    "# nltk package is used for basic natural language processing operations such as removing stopwords, tokenizing words, stemming and\n",
    "# lemmatization.\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# gensim package is applied for LDA modelling and creation of term frequency - inverse document matrices\n",
    "from gensim import corpora \n",
    "from gensim.models import ldamodel\n",
    "from gensim.models import tfidfmodel \n",
    "\n",
    "word_list_toy = []\n",
    "\n",
    "# selecting the 5 created documents\n",
    "for i in range(1,5):\n",
    "    string = str(i)\n",
    "    paragraph = open('doc' + string +'.txt','r')\n",
    "    \n",
    "    for text in paragraph:\n",
    "        structure = text\n",
    "        \n",
    "    # Creating Tokens using word_tokenize function in nltk library\n",
    "    mysentencetokens_sw= nltk.word_tokenize(str(structure))\n",
    "\n",
    "    # normazlizing everything to lower case\n",
    "    looper = 0\n",
    "    for token in mysentencetokens_sw:\n",
    "        mysentencetokens_sw[looper] = token.lower()\n",
    "        looper += 1\n",
    "\n",
    "    # removing stopwords using nltk library with anything less than 2 letters\n",
    "    minlength = 2\n",
    "    mysentencetokens = [token for token in mysentencetokens_sw if \n",
    "                    (not token in stopwords.words('english')) \n",
    "                    and len(token) > minlength]\n",
    "    \n",
    "    #Transforming the words into the stem forms using PorterStemmer function from nltk library\n",
    "    porter = nltk.PorterStemmer()\n",
    "    looper = 0\n",
    "    for token in mysentencetokens:\n",
    "        mysentencetokens[looper] = porter.stem(token)\n",
    "        looper += 1\n",
    "    \n",
    "    #lemmatizing the words so that they are known words from the dictionary using the WordNetLemmatizer function\n",
    "    lmtzr = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "    looper = 0\n",
    "    for token in mysentencetokens:\n",
    "        mysentencetokens[looper] = lmtzr.lemmatize(token)\n",
    "        looper += 1\n",
    "        word_list_toy.append(mysentencetokens)\n",
    "\n",
    "#creating a Dictionary and Corpus from the 10 patent documents\n",
    "dictionary = corpora.Dictionary(word_list_toy) \n",
    "corpus = [dictionary.doc2bow(words) for words in word_list_toy] \n",
    "\n",
    "# creating the term document frequency matrix to obtain distribution of words in a topic across the documents in the corpus\n",
    "term_document_frequency_matrix = tfidfmodel.TfidfModel(corpus) \n",
    "new_corpus = term_document_frequency_matrix[corpus] \n",
    "\n",
    "# applying the LDA model, setting the number of topics to 5 and printing top 10 words on each topic \n",
    "# (Note : default value is top ten words for show_topics command)\n",
    "find_topics = ldamodel.LdaModel(new_corpus, id2word=dictionary, num_topics=5, passes = 20) \n",
    "topics = find_topics.show_topics(formatted = True) \n",
    "\n",
    "print dictionary\n",
    "\n",
    "print \"Top ten words and their frequencies\" \n",
    "print \"Topic 1 :\" \n",
    "print topics[0]\n",
    "print\n",
    "\n",
    "print \"Topic 2 :\" \n",
    "print topics[1]\n",
    "print\n",
    "\n",
    "print \"Topic 3 :\" \n",
    "print topics[2]\n",
    "print\n",
    "\n",
    "print \"Topic 4 :\" \n",
    "print topics[3]\n",
    "print\n",
    "\n",
    "print \"Topic 5 :\"\n",
    "print topics[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output of above code (Part 3A)\n",
    "\n",
    "Dictionary(140 unique tokens: [u'particular', u'size-depend', u'comput', u'discoveri', u'enlighten']...)\n",
    "\n",
    "Top ten words and their frequencies\n",
    "\n",
    "Topic 1 :\n",
    "0.126*data + 0.048*learn + 0.032*inform + 0.032*theori + 0.032*model + 0.032*gener + 0.032*statist + 0.032*comput + 0.016*within + 0.016*drawn\n",
    "\n",
    "Topic 2 :\n",
    "0.007*field + 0.007*scienc + 0.007*nanotechnolog + 0.007*physic + 0.007*materi + 0.007*perform + 0.007*engin + 0.007*broad + 0.007*includ + 0.007*scale\n",
    "\n",
    "Topic 3 :\n",
    "0.078*particl + 0.053*bulk + 0.053*nanoparticl + 0.037*properti + 0.029*size + 0.027*unit + 0.027*whole + 0.027*transport + 0.027*accord + 0.027*diamet\n",
    "\n",
    "Topic 4 :\n",
    "0.047*known + 0.040*materi + 0.030*engin + 0.026*chemistri + 0.026*new + 0.024*year + 0.024*wide + 0.024*mineralog + 0.024*nanosci + 0.024*paradigm\n",
    "\n",
    "Topic 5 :\n",
    "0.053*diver + 0.031*molecular + 0.027*equal + 0.027*devic + 0.027*semiconductor + 0.027*self-assembl + 0.027*rang + 0.027*organ + 0.027*natur + 0.027*nanoscal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3B : Calculation of the jargon distance between documents using the method in the Jargon Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3B.1 : Creation of corpus and dictionary for group 1 - NanoScience Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(71 unique tokens: [u'origin', u'diamet', u'properti', u'constant', u'often']...)\n"
     ]
    }
   ],
   "source": [
    "# creating the corpus for the nanoscience_group \n",
    "word_list_nano = []\n",
    "for i in range(1,3):\n",
    "    string = str(i)\n",
    "    paragraph = open('doc' + string +'.txt','r')\n",
    "    for text in paragraph:\n",
    "        structure = text\n",
    "    # Creating Tokens using word_tokenize function in nltk library\n",
    "    mysentencetokens_sw= nltk.word_tokenize(str(structure))\n",
    "    # normazlizing everything to lower case\n",
    "    looper = 0\n",
    "    for token in mysentencetokens_sw:\n",
    "        mysentencetokens_sw[looper] = token.lower()\n",
    "        looper += 1\n",
    "    # removing stopwords using nltk library with anything less than 2 letters\n",
    "    minlength = 2\n",
    "    mysentencetokens = [token for token in mysentencetokens_sw if \n",
    "                    (not token in stopwords.words('english')) \n",
    "                    and len(token) > minlength]\n",
    "    #Transforming the words into the stem forms using PorterStemmer function from nltk library\n",
    "    porter = nltk.PorterStemmer()\n",
    "    looper = 0\n",
    "    for token in mysentencetokens:\n",
    "        mysentencetokens[looper] = porter.stem(token)\n",
    "        looper += 1\n",
    "    #lemmatizing the words so that they are known words from the dictionary using the WordNetLemmatizer function\n",
    "    lmtzr = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "    looper = 0\n",
    "    for token in mysentencetokens:\n",
    "        mysentencetokens[looper] = lmtzr.lemmatize(token)\n",
    "        looper += 1\n",
    "        word_list_nano.append(mysentencetokens)\n",
    "        \n",
    "#creating a Dictionary and Corpus from the nanoscience_group \n",
    "nano_dictionary = corpora.Dictionary(word_list_nano) \n",
    "nano_corpus = [dictionary.doc2bow(words) for words in word_list_nano] \n",
    "\n",
    "print nano_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output of above short code 3B.1\n",
    "\n",
    "Dictionary(71 unique tokens: [u'origin', u'diamet', u'properti', u'constant', u'often']...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3B.2: Creation of Corpus and Dictionary group 2 - DataScience Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(48 unique tokens: [u'program', u'comput', u'although', u'predict', u'knowledg']...)\n"
     ]
    }
   ],
   "source": [
    "# creating the corpus for the datascience_group \n",
    "word_list_data = []\n",
    "for i in range(4,5):\n",
    "    string = str(i)\n",
    "    paragraph = open('doc' + string +'.txt','r')\n",
    "    for text in paragraph:\n",
    "        structure = text\n",
    "    # Creating Tokens using word_tokenize function in nltk library\n",
    "    mysentencetokens_sw= nltk.word_tokenize(str(structure))\n",
    "    # normazlizing everything to lower case\n",
    "    looper = 0\n",
    "    for token in mysentencetokens_sw:\n",
    "        mysentencetokens_sw[looper] = token.lower()\n",
    "        looper += 1\n",
    "    # removing stopwords using nltk library with anything less than 2 letters\n",
    "    minlength = 2\n",
    "    mysentencetokens = [token for token in mysentencetokens_sw if \n",
    "                    (not token in stopwords.words('english')) \n",
    "                    and len(token) > minlength]\n",
    "    #Transforming the words into the stem forms using PorterStemmer function from nltk library\n",
    "    porter = nltk.PorterStemmer()\n",
    "    looper = 0\n",
    "    for token in mysentencetokens:\n",
    "        mysentencetokens[looper] = porter.stem(token)\n",
    "        looper += 1\n",
    "    #lemmatizing the words so that they are known words from the dictionary using the WordNetLemmatizer function\n",
    "    lmtzr = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "    looper = 0\n",
    "    for token in mysentencetokens:\n",
    "        mysentencetokens[looper] = lmtzr.lemmatize(token)\n",
    "        looper += 1\n",
    "        word_list_data.append(mysentencetokens)\n",
    "        \n",
    "#creating a Dictionary and Corpus from the nanoscience_group \n",
    "data_dictionary = corpora.Dictionary(word_list_data) \n",
    "data_corpus = [dictionary.doc2bow(words) for words in word_list_data] \n",
    "\n",
    "print data_dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output of above short code 3B.2 \n",
    "\n",
    "Dictionary(48 unique tokens: [u'program', u'comput', u'although', u'predict', u'knowledg']...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3B.3 Obtaining the frequency Distributions in DataScience group, NanoScience Group and Total group "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(FreqDist({u'materi': 14, u'scienc': 8, u'field': 8, u'engin': 6, u'properti': 5, u'chemistri': 4, u'new': 4, u'known': 4, u'particl': 3, u'nanotechnolog': 3, ...}),\n",
       " FreqDist({u'data': 16, u'learn': 6, u'comput': 4, u'scienc': 4, u'gener': 4, u'theori': 4, u'inform': 4, u'statist': 4, u'model': 4, u'program': 2, ...}),\n",
       " FreqDist({u'materi': 26, u'scienc': 20, u'field': 18, u'data': 16, u'engin': 14, u'new': 8, u'known': 8, u'chemistri': 8, u'properti': 7, u'learn': 6, ...}))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist \n",
    "\n",
    "# extracting all the words in nanodictionary with related frequencies\n",
    "word_list_nano_final = []\n",
    "word_list_nano_final = word_list_nano[1]\n",
    "word_list_nano_final.extend(word_list_nano[50])\n",
    "word_list_nano_final.extend(word_list_nano[100])\n",
    "\n",
    "# extracting all the words in datadictionary with related frequencies\n",
    "word_list_data_final = []\n",
    "word_list_data_final = word_list_data[1]\n",
    "word_list_data_final.extend(word_list_data[25])\n",
    "\n",
    "\n",
    "#extracting total words from all dictionaries\n",
    "word_list_final = []\n",
    "word_list_final = word_list_toy[1]\n",
    "word_list_final.extend(word_list_toy[20])\n",
    "word_list_final.extend(word_list_toy[50])\n",
    "word_list_final.extend(word_list_toy[100])\n",
    "word_list_final.extend(word_list_toy[150])\n",
    "word_list_final.extend(word_list_toy[210])\n",
    "\n",
    "wordfrequency_nano = nltk.FreqDist(word_list_nano_final) # Calculate the frequency of terms in the nano \n",
    "wordfrequency_data = nltk.FreqDist(word_list_data_final) # Calculate the frequency of terms in the data \n",
    "wordfrequency_total = nltk.FreqDist(word_list_final)\n",
    "\n",
    "wordfrequency_nano, wordfrequency_data, wordfrequency_total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output of above short code 3B.3 \n",
    "(FreqDist({u'materi': 14, u'scienc': 8, u'field': 8, u'engin': 6, u'properti': 5, u'chemistri': 4, u'new': 4, u'known': 4, u'particl': 3, u'nanotechnolog': 3, ...}),\n",
    " FreqDist({u'data': 16, u'learn': 6, u'comput': 4, u'scienc': 4, u'gener': 4, u'theori': 4, u'inform': 4, u'statist': 4, u'model': 4, u'program': 2, ...}),\n",
    " FreqDist({u'materi': 26, u'scienc': 20, u'field': 18, u'data': 16, u'engin': 14, u'new': 8, u'known': 8, u'chemistri': 8, u'properti': 7, u'learn': 6, ...}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3B.4 : Implementation of Jargon Distance calculations and printing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shannon Entropy(nanoscience) : 4.78415926774\n",
      "Cross entropy (nanoscience as writer) : 8.84321150814\n",
      "Efficiency of communication : 0.540997946656\n",
      "Cultural Hole (Jargon Distance): 0.459002053344\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "alpha = 0.01\n",
    "Psi = {} \n",
    "Psj = {}\n",
    "for i, j in wordfrequency_total.items(): \n",
    "    for k, l in wordfrequency_nano.items(): \n",
    "        if i in wordfrequency_nano.keys(): \n",
    "            Psi[i] = (1 - alpha) * (float(l)/len(word_list_nano)) + alpha * (float(j)/len(word_list_toy))\n",
    "        else: \n",
    "            Psi[i] = alpha * (float(j)/len(word_list_toy))\n",
    "\n",
    "for m, n in wordfrequency_total.items():\n",
    "    for o, p in wordfrequency_data.items(): \n",
    "        if m in wordfrequency_data.keys():             \n",
    "            Psj[m] = (1 - alpha) * (float(p)/len(word_list_data)) + alpha * (float(n)/len(word_list_toy))\n",
    "        else: \n",
    "            Psj[m] = alpha * (float(n)/len(word_list_toy))\n",
    "\n",
    "# Shannon Entropy function\n",
    "def Shannon_Entropy(H):\n",
    "    E = 0\n",
    "    for word in H: \n",
    "        E += - word*math.log(word, 2)\n",
    "    return E \n",
    "\n",
    "# Cross entropy function\n",
    "def Cross_entropy(Pi, Pj):\n",
    "    CE = 0 \n",
    "    for ki, pi in Pi: \n",
    "        for kj, pj in Pj: \n",
    "            if ki == kj: \n",
    "                CE += - pi * math.log(pj, 2)\n",
    "    return CE \n",
    "\n",
    "# Shannon Entropy of writer nanoscience and reader DataScience\n",
    "H_nanoscience = Shannon_Entropy(Psi.values())\n",
    "print 'Shannon Entropy(nanoscience) : ' + str(H_nanoscience)\n",
    "\n",
    "# Cross Entropy of writer nanoscience and reader datascience\n",
    "Cross_entropy = Cross_entropy(Psi.items(), Psj.items())\n",
    "print 'Cross entropy (nanoscience as writer) : ' + str(Cross_entropy)\n",
    "\n",
    "# Efficiency of Communication\n",
    "Efficiency = H_nanoscience / Cross_entropy\n",
    "print 'Efficiency of communication : ' + str(Efficiency)\n",
    "\n",
    "# Cultural Hole between two topics\n",
    "Cultural_hole = 1 - Efficiency\n",
    "print 'Cultural Hole (Jargon Distance): ' + str(Cultural_hole)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Final output from jargon distance calculations 3B.4\n",
    "Shannon Entropy(nanoscience) : 4.78415926774\n",
    "Cross entropy (nanoscience as writer) : 8.84321150814\n",
    "Efficiency of communication : 0.540997946656\n",
    "Cultural Hole (Jargon Distance): 0.459002053344\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
